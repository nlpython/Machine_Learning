K-近邻算法:
    定义: 如果一个样本在特征空间中的k个最相似(即特征空间中最临近)的样本中大多数属于某一个类别, 则该样本也属于该类别.
    * 选择合适的 k 值:
        k = 1  容易受到异常点的影响
        k取较大值时, 容易收到样本不均衡的影响
    * 距离计算公式:
        欧氏距离  曼哈顿距离  明可夫斯基距离(是前两者的推广)
    * 先对数据进行无量纲化处理(标准化)

    API: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='atuo')
        - n_neighbors: int 可选(默认为5), 即k值
        - algorithm: {'auto', 'ball_tree', 'kd_tree', 'brute'}, 可选用于计算最近邻居的算法: 'ball_tree'
                      将会使用BallTree, 'kd_tree'将使用KDTree, 'auto'将尝试根据传递给fit方法的值来决定最合适的算法.

    案例: 鸢尾花种类预测
    1) 获取数据
    2) 数据集划分
    3) 特征工程
        * 标准化
    4) KNN预估器流程
    5) 模型评估

    优点: 简单, 易于理解
    缺点: k值选择不当容易导致分类精度不准确
         计算量大, 耗费资源多
