3. 特征工程(feature):
    - 数据和特征决定了机器学习的上限, 而模型和算法只是逼近这个上限而已.

    3.1 什么特征工程?
        - 特征工程是使用专业背景知识和技巧处理数据, 使得特征能在机器学习算法上发挥更好的作用的过程, 会直接影响机器学习的效果.

        sklearn  特征工程
        pandas   数据清洗, 数据处理
            特征抽取/提取
            特征预处理
            特征降维

    3.1.1 特征抽取
        机器学习算法 - 统计方法 - 数学公式
        文本类型 -> 数值
        概念: 将任意数据(文本或图像) 转换为可用于机器学习的数字特征

            特征提取: sklearn.feature_extraction
            1) 字典特征提取: sklearn.feature_extraction.DictVectorizer(sparse=True, ...)
                Vector: 向量, 矢量
                    矩阵 matrix 二维数组
                    向量 vector 一维数组

                - DictVectorizer.fit_transform(X)  X:字典或包含字典的迭代器(列表)  返回值:返回sparse矩阵
                    注: 类别型数据 -> one-hot编码
                返回值: 如果实例化的时候 sparse=True 则返回稀疏矩阵
                          (0, 1)	1.0          # 第0行第1列的值为1
                          (0, 3)	37.0
                          (1, 0)	1.0
                          (1, 3)	31.0
                          (2, 2)	1.0
                          (2, 3)	28.0

                        sparse=False 则返回二维数组
                        [[ 0.  1.  0. 37.]
                         [ 1.  0.  0. 31.]
                         [ 0.  0.  1. 28.]]
                稀疏矩阵: 将二维数组中的非0值按位置表示出来, 优点: 二维数组中往往含有非常多的0, 稀疏矩阵的表示方式能够节省内存, 提高加载效率

                应用场景:
                    1) pclass, sex 数据集当中类别特征比较多
                        1. 将数据集的特征->字典类型
                        2. DictVectorizer转换
                    2) 本身拿到的数据就是字典类型

            2) 文本特征抽取: sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
                单词作为特征
                特征: 特征词
                方法1: CountVectorizer(stop_words=[])
                    统计每个样本特征词出现的个数
                    stop_words: 停用词, 通常为不含具体意义的词(is, too...)

                关键词: 在某一类别的文章中, 出现的次数很多, 但是在其他类别的文章中出现很少
                方法2: TfidfVectorizer()    Tf-idf文本特征提取
                    主要思想: 如果某个词或短语在一篇文章中出现的概率较高, 并且在其他文章中出现的很少, 则认为此词或短语具有很好的类别区分能力, 适合用来分类.
                    作用: 用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度.

                    TF - 词频(term frequency, tf) = 该词出现次数 / 总词次数
                    IDF - 逆向文档频率(inverse document frequency, idf) = lg(总文件数目 / 包含该词语文件数目)
                    Tf-idf = TF * IDF  (越大越说明该词区分能力越强, 越可能是关键词)

                    API: sklearn.featurel_extraction.text.TfidfVectorizer(stop_words=[])
                    - TfidfVectorizer.fit_transform(X)
                        * X:文本或者包含文本的字符串的可迭代对象
                        * 返回值: 返回sparse矩阵
