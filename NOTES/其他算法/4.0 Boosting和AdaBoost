Boosting(进化式集成学习):
    学习器会随着学习的积累从弱到强
    代表算法: Adaboost, GBDT, XGBoost, LightGBM
    
    * Bagging和Boosting集成的区别:
        1. 数据方面:
            - Bagging: 对数据进行采样训练
            - Boosting: 根据前一轮学习结果调整数据的重要性
        2. 投票方面:
            - Bagging: 所有学习器平权投票
            - Boosting: 对学习器加权投票
        3. 学习顺序:
            - Bagging: 学习是并行的, 每个学习器没有依赖关系
            - Boosting: 学习是串行的, 学习有先后顺序
        4. 主要作用:
            - Bagging: 主要用于提高泛化性能(解决过拟合, 也可以说降低方差)
            - Boosting: 主要用于提高训练精度(解决欠拟合, 也可以说降低偏差)
    
    * AdaBoost步骤:
        1. 初始化训练数据权重相等, 训练第一个学习器.
            - 该假设每个训练样本在基分类器的学习中作用相同, 这一假设可以保证第一步能够在原始数据上学习基本分类器H1(x)
        2. AdaBoost反复学习基本分类器, 在每一轮m = 1, 2, ... M顺次的执行下列操作:
            - 在权值分布为Dt的训练数据上, 确定基分类器
            - 计算该学习器在训练数据中的错误率
            - 计算该学习器的投票权重
            - 根据投票权重, 对训练数据重新赋权
            - 重复执行上述步骤, m次
        3. 对m个学习器进行加权投票      
        
        
        API: sklearn.ensemble import AdaBoostClasssifier (用的少)  
        