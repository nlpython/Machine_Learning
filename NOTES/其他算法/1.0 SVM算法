SVM(Supported vector machine, 支持向量机):
    目的: 找到一个超平面, 使样本分为两类, 并且间隔要大.
    svm能够执行线性或非线性分类, 回归, 甚至是异常值的检测任务. 它是机器学习领域最受欢迎的模型之一, 非常适合中小型复杂数据集的分类.

    * API: sklearn.svm.SVC     from sklearn import svm
    * 算法原理:
        空间上的训练集为  T = {(x1, y1), (x2, y2)...(x3, y3)}  xi为实数, yi为-1或1
            - xi为第i个实例(样本)
            - yi为xi的标记, 1为正例, -1为负例

        - 线性可分支持向量机:
            给定了上面提出的线性可分训练数据集, 通过间隔最大化得到分离超平面为 y(x) = W.T * g(x) + b
            相应的分类决策函数为: f(x) = sign(W.T * g(x)) + b)
            其中g(x)为确定的特征空间转换函数, 它的作用是将x映射到更高的维度, 它有一个以后常见的专有称号"核函数"

            那么问题转化为求一组参数(W, b), 使其构建的超平面函数能够最优地分离两个集合.
            - 样本中任意点x到超平面地距离可写成
                    r = |W.T*x + b| / ||W||


    SVM用于回归:
        svm回归是让尽可能多的实例位于预测线上, 同时限制间隔违例(也就是不在预测线距上的实例). 线距的宽度由超参数控制.
        API: svm.SVR

    SVM综述:
        svm既可用于分类(二/多分类), 也可用于回归和异常值检测.
        svm具有良好的鲁棒性, 对未知数据拥有很强的泛化能力, 特别是在数据量较少的情况下, 相较其他传统机器学习算法具有更优的性能.

        流程:
            1.对样本数据进行归一化
            2.应用核函数对样本进行映射(最常采用的核函数是RBF和Linear, 在样本线性可分时, Linear效果更好)
            3.用cross-validation和grid-search对超参数进行优选
            4.用最优参数训练得到模型
            5.测试

        API: sklearn中支持主要三种方法: SVC, NuSVC, LinearSVC, 扩展为三个支持向量回归方法: SVR, NuSVR, LinearSVR
            - SVC和NuSVC方法基本一致, 唯一的区别就是损失函数的度量方式不同
            - LinearSVC是实现线性核函数的支持向量分类, 没有kernel参数

            1.SVC:  sklearn.svm.SVC(C=1.0, kernel='rbf', degree3, coef0=0.0, random_state=None)
                - C越大, 惩罚力度越大, 希望松弛变量接近0, 即对误分类的乘法增大, 趋向于对训练集全分对的情况, 这样会出现
                  训练集测试时准确率很高, 但泛化能力弱, 容易导致过拟合.
                  C越小, 容错能力增强, 泛化能力越强, 但容易导致欠拟合.
                - kernel: 算法中采用核函数类型
                - degree:
                    - 当指定kernel为'poly'时, 表示选择的多项式的最高次数, 默认为三次.
                    - 当kernel不为'poly'时, 则忽略.
                - coef0: 核函数常数值(y = kx + b中的b值)
                    - 只有'poly'和'sigmoid'核函数有, 默认值为0

            2.NuSVC: sklearn.svm.NuSVC(nu=0.5)
                - nu: 训练误差部分的上限和支持向量部分的下限, 取值在(0, 1)之间, 默认为0.5

            3.LinearSVC: sklearn.svm.LinearSVC(penalty='l2', loss='sqaured_hinge', dual=True, C=1.0)
                - penalty: 正则化参数
                - loss: 损失函数类型
                - dual: 是否转化为对偶问题求解, 默认时True
                - C: 惩罚系数, 类似于正则化系数

    SVM优缺点:
        优点:
            - 在高维空间中非常高效
            - 即使在数据维度比样本大的情况下仍然有效
            - 在决策函数(称为支持向量)中使用训练集的子集, 因此它也是高效利用内存的
            - 通用性: 不同的核函数与特定的决策函数一一对应
        缺点:
            - 如果特征数量比样本大得多, 在选择核函数时要避免过拟合
            - 对缺失数据敏感
            - 对于核函数的高维映射解释力不强

