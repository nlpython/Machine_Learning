GDBT(Gradient Boosting Decision Tree, 梯度提升树):
    在传统机器学习中, GBDT算得上TOP3的算法, 想要理解GBDT的真正含义, 就必须理解GBDT中Gradient Boosting Decision Tree分别是什么.

    1. Decision Tree: CART回归树
        首先, GBDT使用的决策树是CART回归树, 无论是处理回归问题还是二分类以及多分类, GBDT使用的决策树都是CART回归树.
            - 为什么不用CART分类树?
                因为GBDT每次迭代要拟合的是梯度值, 是连续值所以要用回归树

            对于回归树算法来说最重要的是寻找最佳的划分点, 那么回归树中的可划分点包含了所有特征的所有可取的值.
            在分类树中最佳划分点的判别标准是熵或者基尼系数, 都是用纯度来衡量的, 但是在回归树中的样本标签是连续数值, 所以再使用熵之类的
            指标不再合适, 取而代之的是平方误差, 它能很好的评判拟合程度.

    2. Gradient Boosting: 拟合负梯度
        梯度提升树(Gradient Boosting)是提升树(Boosting Tree)的一种改进算法, 所以在将梯度提升树之前先来说一下提升树:
            例子: 假如一个人有30岁, 我们首先用20岁去拟合, 发现有10岁损失, 这是我们用6岁去拟合剩下的损失, 发现差距还有4岁.第三轮
                 我们用3岁拟合剩下的损失, 差距就只有一岁了. 如果迭代轮数无穷无尽, 就会不断逼近真实值, 最后将每次迭代的拟合岁数加
                 在岁数加在一起, 就是模型输出的结果

        当损失是平方损失和指数损失函数时, 梯度提升树每一步优化都是很简单的, 但是对于一般损失函数而言, 往往每一步都不容易.
        针对这一问题, Friedman提出了梯度提升树算法, 这是利用最速下降的近似算法, 其关键是利用损失函数的负梯度作为提升树
        算法中的残差的近似值.

    3. GBDT算法原理:
